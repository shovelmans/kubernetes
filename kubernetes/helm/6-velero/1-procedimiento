ðŸ“’ GuÃ­a: Backups y Restores con Velero en DigitalOcean Kubernetes
1ï¸âƒ£ PreparaciÃ³n en DigitalOcean

Crear un Space (bucket) â†’ ej: backupkubernetesjorge en regiÃ³n sfo3.

En API â†’ Spaces access keys, generar una Access Key y Secret Key.

Ejemplo:

Access Key ID: DO00EXEZZKBET8NTMKYL
Secret Key   : my4RWu7PaNkXMHOobasPITYXI9vBLwNymrmj4tM6hf0

2ï¸âƒ£ Crear secreto de credenciales para Velero

Velero espera un archivo estilo AWS en /credentials/cloud.

Crear archivo temporal en tu mÃ¡quina:

cat > credentials-velero <<EOF
[default]
aws_access_key_id=DO00EXEZZKBET8NTMKYL
aws_secret_access_key=my4RWu7PaNkXMHOobasPITYXI9vBLwNymrmj4tM6hf0
EOF


Crear el secreto en Kubernetes:

kubectl create secret generic cloud-credentials \
  --namespace velero \
  --from-file=cloud=credentials-velero

3ï¸âƒ£ Instalar Velero con Helm

AÃ±adir repo de charts:

helm repo add vmware-tanzu https://vmware-tanzu.github.io/helm-charts
helm repo update


Crear archivo values.yaml:

configuration:
  backupStorageLocation:
    - name: default
      provider: aws
      bucket: backupkubernetesjorge
      default: true
      config:
        region: sfo3
        s3ForcePathStyle: "true"
        s3Url: https://sfo3.digitaloceanspaces.com
  volumeSnapshotLocation: []   # desactivamos snapshots DO, solo backups en Spaces

credentials:
  useSecret: true
  existingSecret: cloud-credentials

initContainers:
  - name: velero-plugin-for-aws
    image: velero/velero-plugin-for-aws:v1.8.2
    volumeMounts:
      - mountPath: /target
        name: plugins

deployNodeAgent: true


Instalar Velero:

helm install velero vmware-tanzu/velero \
  --namespace velero --create-namespace \
  -f values.yaml


Verificar:

kubectl get pods -n velero


DeberÃ­as ver velero-xxxx y node-agent-xxxx en estado Running.

4ï¸âƒ£ Crear un backup del namespace monitoring

Ejemplo backup-monitoring.yaml:

apiVersion: velero.io/v1
kind: Backup
metadata:
  name: backup-monitoring
  namespace: velero
spec:
  includedNamespaces:
    - monitoring
  ttl: 240h0m0s
  defaultVolumesToFsBackup: true


Aplicar:

kubectl apply -f backup-monitoring.yaml


Ver estado:

kubectl get backups -n velero
kubectl describe backup backup-monitoring -n velero


âœ… Cuando aparezca Phase: Completed, el backup estÃ¡ guardado en tu Space (backups/ + kopia/).

5ï¸âƒ£ Probar un restore en otro namespace

Ejemplo restore-monitoring-test.yaml:

apiVersion: velero.io/v1
kind: Restore
metadata:
  name: restore-monitoring-test
  namespace: velero
spec:
  backupName: backup-monitoring
  namespaceMapping:
    monitoring: monitoring-restore


Aplicar:

kubectl apply -f restore-monitoring-test.yaml


Verificar:

kubectl get all -n monitoring-restore
kubectl get pvc -n monitoring-restore


ðŸ‘‰ El PVC de Grafana se restaurarÃ¡ con datos.
ðŸ‘‰ El Service puede que no se restaure si el NodePort ya estÃ¡ ocupado â†’ crÃ©alo manualmente con otro puerto (ej: 32001).

6ï¸âƒ£ Restore real en caso de desastre

Si alguna vez borras monitoring, puedes restaurarlo en el mismo namespace:

apiVersion: velero.io/v1
kind: Restore
metadata:
  name: restore-monitoring
  namespace: velero
spec:
  backupName: backup-monitoring

7ï¸âƒ£ Programar backups automÃ¡ticos (opcional)

Ejemplo schedule-monitoring.yaml (backup diario a las 2 AM):

apiVersion: velero.io/v1
kind: Schedule
metadata:
  name: monitoring-backup
  namespace: velero
spec:
  schedule: "0 2 * * *"
  template:
    ttl: 240h0m0s
    includedNamespaces:
      - monitoring
    defaultVolumesToFsBackup: true


Aplicar:

kubectl apply -f schedule-monitoring.yaml

ðŸ“Œ Resumen final

âœ… Configuraste Velero con DigitalOcean Spaces.

âœ… Probaste un backup de monitoring â†’ completado en el bucket.

âœ… Hiciste un restore de prueba en monitoring-restore â†’ Grafana + PVC restaurados.

âš ï¸ Services con NodePort pueden necesitar ajuste manual (por conflicto de puertos).

ðŸš€ Ya puedes confiar en Velero para restaurar tu namespace monitoring en caso de desastre.